# 词向量理解笔记

## 概述

词向量可理解为词汇的向量表示，最传统的表达方式是one-hot，但one-hot在参与运算时会造成维度灾难，而且不支持语义。所以后来有了另一种词向量表示（Distributed Representation），本文说的词向量默认指后一种。

当前所说的词向量算法都是基于**神经概率模型**，在神经概率模型之前，NLP相关的很多任务都通过**统计概率模型**来完成的，可以认为，神经概率模型是在统计概率模型基础上发展来的（有一定的思想借鉴作用）。

后文一些符号预定义：

```
N：字典词数量，数量级范围通常在十万级别
```

## 统计概率模型
统计概率模型，简单理解就是给定一大批语料，通过统计频率，以及运用概率公式，从而获得一张概率表，在进行预测时，直接查表获取概率最大的结果。

```
比如: 计算一个句子的概率，句子S由词（w1,w2,w3,...,wk）组成, 那么 p(S) = p(w1,w2...,wk)

利用Bayes公式链式展开得到： p(S) = p(w1) * p(w2|w1) * p(w3|w1,w2) * ... * p(wk|w1,w2...wk-1)
```

根据[大数定理](https://baike.baidu.com/item/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)，当语料库足够大时，频率可近似概率:

```
即： p(wk|w1,w2...wk-1) = count(w1,w2...wk)/count(w1,w2...wk-1)
```

### n-gram语义

上面的例子在计算概率时，会存在大量的统计运算，尤其是在k很大时。
为了降低计算量，引入了n-gram语义，字面含义就是，一个词出现的概率只跟其前n-1个词有关。

```
n-gram在公式上的体现就是，使用p(wk|w[k-n+1]...w[k-1])代替p(wk|w1,w2...wk-1)。
```

但即使加了n-gram语义，计算量也是相当大，且随着n增大呈指数增长（n一般取3）

```
在预料足够大的情况下，假设词汇量为N，那么预计算的模型参数表量级为N的n次方。
```

除此之外，此模型还有一个平滑化问题：由于是根据统计频率来近似估算概率，再加上语料是由人提供的，语料本身的不全造成有些概率计算会产生倾斜，甚至造成频率为0的场景，但频率=0并不代表概率=0，而平滑化就是解决这个问题的。 

```
对于上述概率计算，可以用另一种方式看待，令p(wk|w[k-n+1]...w[k-1]) = p(wk|Context(wk))

则p(S) = p(w1|Content(w1)) * p(w2|Context(w2)) * ... * p(wk|Context(wk))  --- 公式1

如果p(wk|Context(wk))能根据函数直接算出来，就最好不过了，而神经概率模型就是这么做的。
```


## 神经概率模型

神经概率模型是基于神经网络训练得到的，对于词向量而言，最经典的一篇论文是Bengio在2003年发表的[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)，其后也有一系列相关研究，包括google的word2vec项目。

### A Neural Probabilistic Language Model 论文模型概述

既然是神经概率模型，自然有神经网络了，下图是[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)中的神经网络模型。

![picture](https://raw.githubusercontent.com/iwoods100/machine-learning-documents/master/static/word_vector/model1.jpeg)

图中的每一对 (Context(w),w) 都是一个训练样本，Context(w)是一个list，包含了w的前n-1个词的词向量。

```
作为输入的词向量在一开始其实是未知的，所以看上去比较奇特，因为一般的神经网络的输入都是明确的。
在此模型中，一开始这些词向量是随机初始化的，在训练这个网络模型时，除了训练模型权重参数，也会训练更新词向量。
```

[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)论文中神经网络的前向预测过程。如下：

1. 每一个Context(w)中的多个词向量拼接在一起（得到的结果也称投影层），通过第一层权重矩阵(也称隐藏层)后再使用激活函数tanh得到了第一层结果
2. 将1中的结果通过输出层的权重矩阵，得到N维结果（N为字典大小），每一维对应一个词的预测结果，但还没有得到概率
3. 对N维结果进行[softmax归一化](https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0)，得到N个词对应的概率

```
Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量 “压缩”到另一个K维实向量中，
使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。
```
下图是另一张模型图，跟上图含义是一样的，看上去可能更贴合神经网络一点：
![picture](http://ww3.sinaimg.cn/large/6cbb8645gw1f5to0uwydsj216i0ikdi3.jpg)

此模型是标准的神经网络模型，训练算法细节可参考原论文。

这个模型在按照后向传播算法训练时，由于输出层是N维的（意味着输出层有N个感知器，每个样本在训练时都需要全部更新一遍感知器参数），会导致训练性能消耗很大。

### word2vec模型算法

在实际模型训练过程中，主要关心的是性能问题，而[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)模型中最耗性能的就是隐藏层和softmax归一化，
像google的word2vec，干脆就去掉了隐藏层，而且还觉得不够，因为softmax也很慢，因此使用了两种方案来替代softmax，分别是Hierarchical Softmax与Negative Sampling。

词向量作为神经概率模型的输入，本身只是模型训练得到的附属品。通常而言，这里的“模型”通常指两个场景，分别是“CBOW”和“Skip-gram”。

![picture](https://raw.githubusercontent.com/iwoods100/machine-learning-documents/master/static/word_vector/model3.jpeg)

```
结合前面提到的Hierarchical Softmax与Negative Sampling。两个模型乘以两种方法，一共有四种实现。
```

学习【机器学习】方法，主要是指三方面：模型，优化目标，优化算法。

```
我的理解：
在神经网络中，模型指如何定义神经网络层数，激活函数，输入什么，输出什么等等。
优化目标是训练模型的方向，跟损失函数最小化差不多。
优化算法指为了让优化目标达到最优解，所使用的算法。
```

下面在介绍CBOW，Skip-gram时都将从这三个方面入手。

```
CBOW: 输入单个词的上下文，预测词本身，训练样本为(Context(w), w)
Skip-gram: 输入单个词，预测词上下文，训练样本为(w, Context(w))

个人感觉，CBOW在现实意义上感觉还有些作用，Skip-gram的作用意义不是很大，虽然两者都会得到词向量，
但Skip-gram更像是为了词向量而设计出的一种模型。

事实证明，在效果上，Skip-gram比CBOW会更好一些。
```

##### CBOW/Skip-gram模型介绍

关于模型介绍，文章[word2vec-中的数学原理详解](https://github.com/iwoods100/machine-learning-documents/blob/master/static/word_vector/word2vec-%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.pdf)已经详细介绍过了（第四节介绍了基于Hierarchical Softmax实现的两种模型， 第五节介绍了基于Negative Sampling实现的两种模型）。


```
这里的模型都使用了【随机梯度上升】优化算法，作为机器学习中最常见的优化算法，这里简单介绍一下此算法的核心思想。

梯度下降法（Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 
要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。
如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

在单变量的实值函数的情况，梯度只是导数，或者，对于一个线性函数，也就是线的斜率。

对于多变量，如何更新每一个变量，从而使得最后的变量算出来的损失最小？
对每一个变量求偏导，把偏导看作是其梯度，同样的更新逻辑。
```

几点思考：

问

```
1. 为什么用Huffman树？Huffman树是必须的吗？
2. 为什么Skip-gram效果比CBOW好，可能的原因是什么？
3. ns相比hs更慢，为什么？
```

答

```
1. 让最频繁的词训练路径最短，从而让整体训练成本最低，不是必须使用Huffman数，只要是二叉树都可以，只是效率达不到最优。
2. 我的理解：感觉sg更像是为词向量而生，训练更新的词也是单独更新的（cbow是将文本向量的变化 更新加到每一个词上）
3. ns训练参数更多，但效果比hs好一些
```



### fastText word representation

[fastText](https://github.com/facebookresearch/fastText) 是一个支持词向量与文本分类功能的工具，本文只介绍其词向量（word representation）功能。


fastText word representation也包含cbow与skip-gram两种模型，默认使用skip-gram。

前文介绍了word2vec项目，实际上fastText只是在word2vec上增加了character n-grams语义。

#### character n-grams语义
character n-grams与前文的n-gram不一样，指的是单个词内的n-gram，俗称子词，fastText中默认子词的长度限制为：minn=3， maxn=5
举个例子：【吃鸡】的子词如下：

```

./fasttext print-ngrams ./model/all.bin 吃鸡
吃鸡 0.33304 -0.097267 0.034237 0.30877 0.018578 0.60418 -0.99443 0.41294 -0.15867 -0.014715 0.016782 -0.2148 -0.74966 -0.31513 0.17342 -0.24861 0.76043 0.41276 -0.25764 0.2902 -0.72365 0.084045 -0.20415 -0.13327 0.098202 0.13701 0.58651 0.057891 0.77085 0.46884 -0.051243 -0.45863 -0.17717 0.18576 0.13283 -0.29679 -0.10459 -0.069326 0.55458 -0.43109 0.52468 0.73316 0.10947 0.68325 0.57203 0.25059 -0.1906 -0.61154 0.30607 -0.19783 0.30671 -0.077505 -0.090187 0.32242 -0.52572 0.46405 -1.0943 1.3622 0.29269 -0.019292 -0.58621 0.68809 -0.3033 -0.13849 0.11134 -0.35706 0.27645 -0.057726 0.092354 -0.31393 0.097598 -0.3381 -0.15205 0.61793 -0.063439 -0.12106 0.35485 -0.29457 0.11482 -0.36802 -0.019554 0.43765 -1.1892 -0.15133 -0.35799 -0.43366 0.41663 0.13935 -0.20402 0.52457 -0.67969 -0.61472 -0.5774 -0.085566 -0.35183 0.30417 0.062178 -0.49339 0.1292 0.75276
<吃鸡 -0.11045 0.064509 0.045973 -0.59812 0.05758 0.946 -0.80604 -0.24761 0.28214 0.51917 -0.79643 -0.41826 -0.61711 0.25488 0.059725 -0.59474 0.5095 0.39657 0.29344 -0.72945 -0.32776 -0.11446 0.25577 0.090114 0.94694 -0.019388 0.20195 -0.27874 0.3488 0.66178 -0.23248 -0.78061 -0.25268 -0.38108 -0.1721 -0.29914 -0.22714 -0.23632 -0.092 -0.47589 0.76252 0.18557 0.41782 0.43933 0.50013 -0.27061 0.068364 -0.61443 -0.56532 0.10744 -0.054887 0.14858 0.25184 -0.23394 -0.78539 0.62458 -0.78286 0.6057 0.62571 0.20618 0.041336 0.48604 -0.80195 0.041745 0.74237 0.039659 0.39937 0.86537 0.14428 -1.3392 -0.76677 -0.80922 -0.49863 -0.36525 -0.25595 -0.61163 0.17738 0.28821 -0.15188 0.35502 0.039618 0.29114 -1.0265 0.92546 -0.62297 -0.10065 -0.26937 -0.057946 -0.75481 0.23732 -0.59514 -0.18718 -0.056282 -0.1366 0.50888 -0.11085 -0.44736 -0.21571 0.57324 0.054843
<吃鸡> 0.1043 -0.065526 0.55593 0.13034 -0.069343 0.41637 -0.84589 -0.46929 -0.92045 0.38924 0.12019 -0.11384 -0.48797 -0.070097 0.47782 0.056651 0.70753 0.78423 0.13639 0.12393 -0.38713 -0.35055 -0.15517 0.07638 -0.12405 0.27381 0.90129 0.34271 0.53353 0.42526 -0.65244 -1.1501 -0.16383 0.033337 0.16184 -0.51836 -0.73194 -0.44876 0.50867 -0.24132 0.28859 0.19331 -0.56854 1.2027 0.38285 -0.75571 0.26711 0.076393 0.34745 0.12033 0.084737 -0.19424 -0.39302 0.51519 -0.39349 0.57322 -0.24391 0.70079 0.38766 0.18975 0.0078733 0.27602 -0.73359 -0.30931 0.035836 0.48695 0.2452 -0.4534 1.4051 0.15972 0.4737 0.164 -0.40653 1.2598 -0.51264 0.066972 -0.36889 -0.0029096 -0.23837 0.032082 0.10487 0.013571 -1.0892 0.13657 -0.58161 0.17051 0.39531 -0.27963 0.050666 0.58342 -0.50879 -0.6963 -0.30292 0.33505 0.0489 0.047124 -0.32938 -0.75936 0.69413 1.41
吃鸡> 0.34527 -0.077652 0.035013 0.30407 -0.010199 0.64369 -0.97594 0.44273 -0.15756 -0.037683 0.0047556 -0.20837 -0.73867 -0.32818 0.16938 -0.2557 0.78319 0.38433 -0.27978 0.33311 -0.7394 0.098448 -0.23648 -0.12589 0.12682 0.13259 0.58684 0.042705 0.78637 0.49807 -0.029649 -0.43573 -0.16765 0.18722 0.12641 -0.29814 -0.10161 -0.043721 0.56035 -0.4251 0.53596 0.72077 0.092262 0.67258 0.57784 0.21265 -0.20268 -0.6315 0.29185 -0.21547 0.30042 -0.085635 -0.049964 0.33836 -0.52119 0.47952 -1.0942 1.3531 0.29581 -0.0077501 -0.60327 0.67694 -0.27592 -0.13001 0.10609 -0.37629 0.28642 -0.06125 0.092964 -0.31717 0.096484 -0.31005 -0.13359 0.61658 -0.043705 -0.12892 0.34744 -0.29378 0.11304 -0.38031 -0.047817 0.44094 -1.1883 -0.16679 -0.37138 -0.45678 0.42659 0.14369 -0.20054 0.52743 -0.68142 -0.59189 -0.58676 -0.059763 -0.34082 0.30321 0.068104 -0.48021 0.1127 0.73976
```

下面总结了[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)论文的一些主要观点。

```
在某些语言中，使用字符n-gram显得更重要（德语在语法形态上比英语、西班牙语更丰富）
We also observe that the effect of
using character n-grams is significantly more important
for German than for English or Spanish. This
is not surprising since German is morphologically
richer than English or Spanish.


排除掉出现最多的p个词的子词，可提高效果(但性能代价更高)，如果p=总词数量，算法跟sg就一样了。所以fastText词向量算法只是在sg上增加了子词
To improve the efficiency of our model,
we do not use n-grams to represent
the P most frequent words in the vocabulary. There
is a trade-off in the choice of P, as smaller values
imply higher computational cost but better performance.
When P = W, our model is the skip-gram

加上子词后会慢1.5x
on English data, our model with character ngrams
is approximately 1.5× slower to train than the
skip-gram baseline (105k words/second/thread versus
145k words/second/thread for the baseline).

如何评估效果（通过计算Human similarity judgement与cosine similarity的相关系数）
Human similarity judgement. We first evaluate
the quality of our representations by
computing Spearman’s rank correlation coeffi-
cient (Spearman, 1904) between human judgement
and the cosine similarity between the vector representations

cbow与sg无法预测未知词
Some words from these datasets do not appear in
our training data, and thus, we cannot obtain word
representation for these words using the CBOW and
skip-gram baselines.
```

词向量训练实践：

```
训练环境：
loss: ns, 线程数: 15, 除了特别指定的参数外，其它参数均为默认值
Read 354M words（语料总词数）
Number of words:  664422（词典大小）

初始loss：4左右
```
|训练参数|最终loss|训练时长|words/sec/thread|
| ------------- |:-------------:| -----:| -----:|
|dim=100, epoch=5|0.220873|30m|65k|
|dim=100, epoch=15|0.086431|1h30m|65k|
|dim=100, epoch=30|?|?|65k|
|dim=200, epoch=5|0.220477|55m|38k|
|dim=200, epoch=15|0.088025|2h40m|38k|
|dim=200, epoch=30|?|?|38k|
|dim=300, epoch=5|0.222116|1h15m|26k|
|dim=300, epoch=15|?|3h40m|26k|
|dim=300, epoch=30|?|7h10m|26k|


介绍几个重要命令：

# References
[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

word2vec: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

[Word2vec中的数学原理详解](https://wenku.baidu.com/view/042a0019767f5acfa1c7cd96.html)

[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)

fastText: [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)








